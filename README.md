# rag 
RAG系统
├── 检索阶段 (CLIP负责)
│   ├── 文本嵌入：embed_documents()
│   └── 图像特征提取：get_image_features()
│
└── 生成阶段 (Ollama负责)
    ├── 接收问题和检索到的上下文
    └── 生成最终回答

# prompt
 - 指令在大量数据训练的预训练模型下作用不大
 - 让我们一步一步来思考 这个技巧是否普遍有效
 - 模型的能力取决于上下文  如果 GPT-3 在一个简单的逻辑问题上出错，那么它一定无法处理简单的逻辑。这是上下文问题
 - 如何提高复杂任务的可靠性
    本文的其余部分分享了提高大型语言模型在复杂任务上可靠性的技巧。虽然其中一些技巧特定于某些类型的问题，但许多技巧都是基于可以广泛应用于各种任务的通用原则，例如：

    给出更清晰的指令
    将复杂任务分解为更简单的子任务
    构建指令以保持模型在任务上
    提示模型在回答前进行解释
    要求对多种可能的答案进行论证，然后综合
    生成多个输出，然后使用模型选择最佳的一个
    微调定制模型以最大化性能


 # ollama
   - 为什么要用 Modelfile？
Modelfile 的核心作用是：把您每次都要口头交代的“个性化要求”，直接写成一份永久的“定制菜单”**。没有 Modelfile（每次都口头交代）：
   - --temperature 0.9 --seed 42   将其设置为特定数字将使模型为相同的提示生成相同的文本



   # Ollama vs 直接使用模型：厨房比喻详解

## 核心比喻

| 概念 | 对应关系 |
|------|-----------|
| **直接使用模型** | 手工厨房 - 从零开始准备所有食材和工具 |
| **使用Ollama** | 全自动厨房 - 预制好了一切，一键烹饪 |

## 功能对比表

| 方面 | 直接使用模型（"手工厨房"） | 使用Ollama（"全自动厨房"） |
|-----|--------------------------|----------------------------|
| **获取模型** | 手动从Hugging Face等网站下载，需要处理文件路径、格式等 | 使用 `ollama pull` 命令，直接从集成的模型库中获取，无需关心存储细节 |
| **环境配置** | 需要自行安装PyTorch/CUDA/Transformers等库，极易出现版本冲突 | **一键配置**：Ollama自带完整运行环境，无需担心依赖问题 |
| **硬件要求** | 需要自行理解和配置GPU驱动、CUDA、显存占用等 | **智能适配**：Ollama自动检测硬件并选择最优运行方案 |
| **运行方式** | 需要编写Python脚本，加载模型、处理token、管理上下文等 | **标准化操作**：`ollama run` 命令统一所有模型的启动方式 |
| **适用场景** | **研究、二次开发、极限性能优化**（需要深入了解模型内部机制） | **快速启动、日常使用、应用集成**（面向最终用户） |
| **维护成本** | 高 - 需要不断关注模型更新、库版本兼容性等问题 | 低 - Ollama团队负责底层维护，用户始终获得最佳体验 |
| **扩展能力** | 灵活但复杂 - 可以自由修改模型架构和推理逻辑 | **便捷扩展** - 轻松切换不同模型，无需修改应用代码 |
| **产出速度** | 慢 - 大量时间花在环境搭建和调试上 | 快 - 专注业务逻辑，而非技术实现细节 |

## 实际体验差异

### 🔧 直接使用模型（手工厨房）
```bash
# 感觉像是：
- 先去市场买各种原材料（下载模型权重）
- 准备好所有厨具（安装深度学习框架）
- 学习烹饪技巧（理解模型工作原理）
- 花费数小时准备一顿饭

### ⚡ 使用Ollama（全自动厨房）
```bash
# 感觉像是：
ollama pull llama2    # 放入预制食材
ollama run llama2     # 按下"开始烹饪"按钮