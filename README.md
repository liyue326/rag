# RAG 系统架构
RAG 系统
├── 检索阶段 (CLIP 负责)
│   ├── 文本嵌入：`embed_documents()`
│   └── 图像特征提取：`get_image_features()`
└── 生成阶段 (Ollama 负责)
    ├── 接收问题与检索到的上下文
    └── 生成最终回答

---

# Prompt 工程反思
- **指令在大量数据训练的预训练模型下作用不大**  
- **“让我们一步一步来思考”是否普遍有效？**  
- **模型能力取决于上下文**  
  &gt; 若 GPT-3 在简单逻辑题上出错，未必是“无法处理简单逻辑”，而是**上下文问题**。  
- **如何提高复杂任务的可靠性**  
   LLM 可靠性的技巧，通用原则包括：  
  1. 给出更清晰的指令  
  2. 将复杂任务拆分为更简单的子任务  
  3. 构建指令以保持模型聚焦任务  
  4. 提示模型在回答前先解释  
  5. 要求对多种可能答案进行论证，再综合  
  6. 生成多个输出，再用模型选最佳  
  7. 微调定制模型以最大化性能

---

# Ollama 使用笔记
## 为什么要用 Modelfile？
Modelfile 的核心作用：把每次都要口头交代的“个性化要求”写成一份**永久定制菜单**。  
无 Modelfile（每次口头交代）→ 重复、易错、难复现。  

示例固化参数  
```bash
--temperature 0.9 --seed 42




| 方面       | 直接使用模型（手工厨房）                       | 使用 Ollama（全自动厨房）          |
| -------- | ---------------------------------- | ------------------------- |
| **获取模型** | 手动下载权重、处理路径/格式                     | `ollama pull` 一键拉取，无需关心存储 |
| **环境配置** | 自行安装 PyTorch/CUDA/Transformers，易冲突 | 自带完整运行环境，零依赖烦恼            |
| **硬件要求** | 自行配置 GPU 驱动、CUDA、显存                | 智能检测硬件，自动最优方案             |
| **运行方式** | 写 Python 脚本加载模型、管理上下文              | `ollama run` 统一启动，标准化操作   |
| **适用场景** | 研究、二次开发、极限性能优化                     | 快速启动、日常使用、应用集成            |
| **维护成本** | 高：需关注模型与库版本兼容性                     | 低：Ollama 团队负责底层维护         |
| **扩展能力** | 灵活但复杂：可改架构与推理逻辑                    | 便捷扩展：零代码切换模型              |
| **产出速度** | 慢：大量时间花在搭建与调试                      | 快：专注业务逻辑，无需关注细节           |


使用 Ollama
ollama pull llama2    # 放入预制食材
ollama run llama2     # 按下“开始烹饪”按钮